# -*- coding: utf-8 -*-
"""PROYECTO BIG DATA GRUPO 7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hAkG64bXv-BuhfkH0_3qH2lx-pIUcLTK

# GRUPO 7. BIG DATA. PREDICCIÓN DATOS CALIDAD DEL AIRE Y CLIMATOLÓGICOS
---

#  1.Carga de los datos
"""

# Commented out IPython magic to ensure Python compatibility.
# Cargamos las librerias que vamos a usar.
import pandas as pd
import datetime
from fbprophet import Prophet
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
# %matplotlib inline
from scipy.stats import norm
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

import pandas as pd
from google.colab import files

uploaded = files.upload()

# Carga dataframe
datos_original = pd.read_csv('datosdefinitivos.csv',sep=';')

"""**VISUALIZACIÓN DATAFRAME**"""

# Consultamos el dataframe
# Magnitud 81: Velocidad_viento ; Magnitud 82: Direccion_viento ; Magnitud 83: Temperatura ;
#Magnitud 86: Humedad_relativa ;  Magnitud 87: Presion_atmosferica ;  Magnitud 88: Radiaccion_solar ;
#Magnitud 89: Precipitacion
datos_original.head()

# Consultamos datos estadísticos comunes.
datos_original.describe()

# Consultamos la informacion del dataframe
datos_original.info()

# Consultamos las columnas.
datos_original.columns

# Renombramos columnas magnitudes
datos_original = datos_original.rename(columns={"valor_magnitud_81": "Velocidad_viento", "valor_magnitud_82": "Direccion_viento", "valor_magnitud_83": "Temperatura", "valor_magnitud_86": "Humedad_relativa", "valor_magnitud_87": "Presion_atmosferica", "valor_magnitud_88": "Radiaccion_solar", "valor_magnitud_89": "Precipitacion"})
datos_original.head()

# Consultamos el tipo de datos de cada columna.
datos_original.dtypes

#Consultamos Valores faltantes
datos_original.isnull().sum()

#Eliminamos valores Nulos
datos_filtrados = datos_original.dropna()
datos_filtrados.isnull().sum()

datos_filtrados.loc[:, (datos_filtrados == 0).all()]
datos_filtrados =datos_filtrados[datos_filtrados['ica_parcial'] !=0]

datos_filtrados.head()

"""# 2.Visualización de los datos

## a) ICA PARCIAL
"""

# Suma de ICA agrupado por Hora y Magnitudes 
fig, ax = plt.subplots(figsize=(15,7))
datos_filtrados.groupby(['hora','descripcion_magnitud']).sum()['ica_parcial'].unstack().plot(ax=ax)

print(datos_filtrados['descripcion_magnitud'].value_counts())
plt.figure(figsize=(12,5))
sns.countplot(datos_filtrados['descripcion_magnitud'])
plt.show()

# Valor ICA por Magnitudes
ax = sns.lineplot(x = 'fecha', y = 'ica_parcial', hue = 'descripcion_magnitud', data = datos_filtrados)
ax.set(xlabel = 'Fecha', ylabel='ICA PARCIAL')
plt.show()

"""**ICA**   CALIDAD DEL AIRE Índice de Calidad del Aire(µg/m3 )"""

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("ica_parcial", "ica_parcial", kind="scatter")
plt.show()

# Consultamos los valores que más se repiten en ICA y los que menos.
ica = datos_filtrados['ica_parcial'].value_counts()
ica

# Consultamos el promedio del valor de ICA
ica.mean()

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['ica_parcial'].skew())
print("Curtosis: %f" % datos_filtrados['ica_parcial'].kurt())

# Convertimos fecha en datetime
df = datos_filtrados.loc[:, ["fecha","ica_parcial"]]
df['fecha'] = pd.DatetimeIndex(df['fecha'])
df.dtypes

#Mostramos histograma con valores ica parcial
plt.figure(figsize=(12,5))
plt.title("ICA")
ax = sns.distplot(datos_filtrados["ica_parcial"], color = 'y')

# Mostramos gráfico tipo violín con datos ica parcial
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['ica_parcial']).ica_parcial)

# Mostramos valores medios por meses de ica parcial
df.groupby(df['fecha'].dt.strftime('%B'))['ica_parcial'].mean().sort_values()

# Creamos columnas con el mes, dia y año
df['mes'] = df['fecha'].dt.month
df['dia'] = df['fecha'].dt.day
df['año'] = df['fecha'].dt.year
df

# Mostramos los meses del análisis que son Enero, Febreo, Marzo, Abril y Mayo
df['mes'].unique()

# Media mensual ICA
df.groupby(['mes']).mean()[['ica_parcial']]

# Graficamos promedio ica por meses del 1(enero) al 5(mayo) 
dategroup=df.groupby('mes').mean()
plt.figure(figsize=(12,5))
dategroup['ica_parcial'].plot(x=df.fecha)
plt.title('ICA PROMEDIO MES')

# Valor ICA por HORAS (24H)
dategroup=datos_filtrados.groupby('hora').mean()
plt.figure(figsize=(12,5))
dategroup['ica_parcial'].plot(x=datos_filtrados.fecha)
plt.title('ICA')

"""## b) Velocidad del viento"""

#Mostramos histograma con valores velocidad de viento
plt.figure(figsize=(12,5))
plt.title("Velocidad del viento")
ax = sns.distplot(datos_filtrados["Velocidad_viento"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Velocidad_viento'].skew())
print("Curtosis: %f" % datos_filtrados['Velocidad_viento'].kurt())

# Mostramos gráfico tipo violín con datos Velocidad_viento
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Velocidad_viento']).Velocidad_viento)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Velocidad_viento", "Velocidad_viento", kind="scatter")
plt.show()

# Distribución de los valores de la magnitud Velocidad del viento
datos_filtrados[['Velocidad_viento']].plot(kind='hist',bins=[0,0.5,1.0,2,2.0,2.5],rwidth=0.8)
plt.show()

# Creamos dataframe con viento y Convertimos fecha en datetime
viento = datos_filtrados.loc[:, ["fecha","Velocidad_viento"]]
viento['fecha'] = pd.DatetimeIndex(viento['fecha'])
viento.dtypes

# Consultamos valores del la magnitud viento
viento.describe()

"""## c) Dirección del viento"""

#Mostramos histograma con valores dirección del viento
plt.figure(figsize=(12,5))
plt.title("Dirección del viento")
ax = sns.distplot(datos_filtrados["Direccion_viento"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Direccion_viento'].skew())
print("Curtosis: %f" % datos_filtrados['Direccion_viento'].kurt())

# Mostramos gráfico tipo violín con datos Direccion_viento
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Direccion_viento']).Direccion_viento)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Direccion_viento", "Direccion_viento", kind="scatter")
plt.show()

# Creamos dataframe con Direccion_viento y Convertimos fecha en datetime
direccion = datos_filtrados.loc[:, ["fecha","Direccion_viento"]]
direccion['fecha'] = pd.DatetimeIndex(direccion['fecha'])
direccion.dtypes

# Consultamos valores del la magnitud viento
direccion.describe()

"""## d) Temperatura"""

#Mostramos histograma con valores Temperatura
plt.figure(figsize=(12,5))
plt.title("Temperatura")
ax = sns.distplot(datos_filtrados["Temperatura"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Temperatura'].skew())
print("Curtosis: %f" % datos_filtrados['Temperatura'].kurt())

# Mostramos gráfico tipo violín con datos Temperatura
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Temperatura']).Temperatura)

w# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Temperatura", "Temperatura", kind="scatter")
plt.show()

"""## e) Humedad relativa"""

#Mostramos histograma con valores Humedad relativa
plt.figure(figsize=(12,5))
plt.title("Humedad_relativa")
ax = sns.distplot(datos_filtrados["Humedad_relativa"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Humedad_relativa'].skew())
print("Curtosis: %f" % datos_filtrados['Humedad_relativa'].kurt())

# Mostramos gráfico tipo violín con datos Humedad relativa
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Humedad_relativa']).Humedad_relativa)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Humedad_relativa", "Humedad_relativa", kind="scatter")
plt.show()

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Humedad_relativa'].skew())
print("Curtosis: %f" % datos_filtrados['Humedad_relativa'].kurt())

"""## f) Presión atmosferica"""

#Mostramos histograma con valores Presion atmosferica 
plt.figure(figsize=(12,5))
plt.title("Presion_atmosferica")
ax = sns.distplot(datos_filtrados["Presion_atmosferica"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Presion_atmosferica'].skew())
print("Curtosis: %f" % datos_filtrados['Presion_atmosferica'].kurt())

# Mostramos gráfico tipo violín con datos Presion atmosferica 
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Presion_atmosferica']).Presion_atmosferica)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Presion_atmosferica", "Presion_atmosferica", kind="scatter")
plt.show()

"""## g) Radiacción Solar"""

#Mostramos histograma con valores Radiacción Solar
plt.figure(figsize=(12,5))
plt.title("Radiaccion_solar")
ax = sns.distplot(datos_filtrados["Radiaccion_solar"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Radiaccion_solar'].skew())
print("Curtosis: %f" % datos_filtrados['Radiaccion_solar'].kurt())

# Mostramos gráfico tipo violín con datos Radiaccion solar  
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Radiaccion_solar']).Radiaccion_solar)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Radiaccion_solar", "Radiaccion_solar", kind="scatter")
plt.show()

"""## h) Precipitación"""

#Mostramos histograma con valores Precipitacion
plt.figure(figsize=(12,5))
plt.title("Precipitacion")
ax = sns.distplot(datos_filtrados["Precipitacion"], color = 'y')

# Asimetría y curtosis:
print("Asimetría: %f" % datos_filtrados['Precipitacion'].skew())
print("Curtosis: %f" % datos_filtrados['Precipitacion'].kurt())

# Mostramos gráfico tipo violín con datos Precipitacion   
fig, ax = plt.subplots()
fig.set_size_inches(10,5)
sns.violinplot(datos_filtrados.dropna(subset = ['Precipitacion']).Precipitacion)

# Consultamos la concentracion de valores y outliers 
my_plot = datos_filtrados.plot("Precipitacion", "Precipitacion", kind="scatter")
plt.show()

"""# 3.Preparacion de los datos"""

# Creamos campo binario ica para aquellos valores que superen percentil 75.
datos_filtrados['ica'] = np.where(datos_filtrados['ica_parcial']>=60, 0, 1) # Marcamos con 0 los que superen valor 60
datos_filtrados.head(5)

# Normalizamos datos de ICA y magnitudes
from sklearn import preprocessing
values = ['ica_parcial', 'Velocidad_viento', 'Direccion_viento', 'Temperatura', 'Humedad_relativa','Presion_atmosferica','Radiaccion_solar','Precipitacion']
x = datos_filtrados[values] #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
datos_normalizados = pd.DataFrame(x_scaled)
datos_normalizados.columns = ['ica_parcial', 'Velocidad_viento', 'Direccion_viento', 'Temperatura', 'Humedad_relativa','Presion_atmosferica','Radiaccion_solar','Precipitacion']
datos_normalizados.insert(0, "Fecha", datos_filtrados["fecha"]) # incluimos fecha
datos_normalizados.head()

#datos_normalizados.describe()
datos_normalizados.dtypes

# Correlacion entre variables
datos_normalizados.corr()

### Consultamos diagramas box y whisker (cajas y bigotes)
boxplot = datos_normalizados.boxplot(grid=False, rot=45, fontsize=12)

# Matriz de correlación de las magnitudes
corrmat = datos_normalizados.corr(method='pearson')
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

#Correlacion con datos
f,ax = plt.subplots(figsize=(9, 9))
sns.heatmap(datos_normalizados.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()

"""# 4.Evaluación de algoritmos"""

# Preparamos dataframe
datos_normalizados = datos_normalizados.drop(['Fecha'], axis=1) # eliminamos fecha
datos_normalizados.insert(1, "ica", datos_filtrados["ica"]) # incluimos ica binario >60 1
datos_normalizados.head(5)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve 
from sklearn.metrics import roc_auc_score

X = datos_normalizados.drop('ica', axis=1)
y = datos_normalizados.ica

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

y

"""## 1. K-NN"""

# Representación gráfica de los datos.
x = datos_normalizados['Temperatura'].values
y = datos_normalizados['ica_parcial'].values
plt.xlabel('Temperatura')
plt.ylabel('ica_parcial')
plt.title('Temperatura vs. ica_parcial')
plt.plot(x,y,'o',markersize=1)

knMod = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,
                             metric='minkowski', metric_params=None)

knMod.fit(X_train, y_train)
knMod.score(X_test, y_test)

"""## 2. Regresión Logística"""

glmMod = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True,
                            intercept_scaling=1, class_weight=None, 
                            random_state=None, solver='liblinear', max_iter=100,
                            multi_class='ovr', verbose=2)

glmMod.fit(X_train, y_train)
glmMod.score(X_test, y_test)

test_labels=glmMod.predict_proba(np.array(X_test.values))[:,1]
roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)

"""## 3. AdaBoost"""

adaMod = AdaBoostClassifier(base_estimator=None, n_estimators=200, learning_rate=1.0)

adaMod.fit(X_train, y_train)
adaMod.score(X_test, y_test)

test_labels=adaMod.predict_proba(np.array(X_test.values))[:,1]
roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)

"""## 4. GradientBoosting"""

gbMod = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,
                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, 
                                   max_depth=3,
                                   init=None, random_state=None, max_features=None, verbose=0)

gbMod.fit(X_train, y_train)
gbMod.score(X_test, y_test)

test_labels=gbMod.predict_proba(np.array(X_test.values))[:,1]
roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)

"""## 5. RandomForest"""

rfMod = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,
                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',
                               max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, 
                               random_state=None, verbose=0)

rfMod.fit(X_train, y_train)
rfMod.score(X_test, y_test)

test_labels=rfMod.predict_proba(np.array(X_test.values))[:,1]
roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)

"""# 5.Matriz de Confusión

La Matriz de Confusión es una de las métricas más intuitivas y sencillas que se utiliza para encontrar la precisión y exactitud del modelo. 

![texto alternativo](https://i1.wp.com/live.staticflickr.com/4849/33154606388_7a72bc6c14_c.jpg?resize=800%2C249&ssl=1)
"""

# Creamos array datos Train y Test
data = {'y_Actual':   y_train,
        'y_Predicted': y_test
        }

# Creamos dataFrame con columnas del array
datos = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])

# Seleccionamos numero filas deseadas y convertimos NaN a 0
datostop= datos.head(20000)
datostop = datostop.fillna(0)

# Dibujamos la matrix de Confusión
confusion_matrix = pd.crosstab(datostop['y_Actual'], datostop['y_Predicted'], rownames=['Actual'], colnames=['Predicción'])

sns.heatmap(confusion_matrix, annot=True)
plt.show()

# Resultados matrix confusion
results = confusion_matrix(datostop['y_Actual'], datostop['y_Predicted'])

print ('Confusion Matrix :')
print(results) 
print ('Accuracy Score :'),accuracy_score(datostop['y_Actual'], datostop['y_Predicted']) 
print (')Report : ')
print( classification_report(datostop['y_Actual'], datostop['y_Predicted']) )

"""# 6.Curva ROC

La curva AUC-ROC es la métrica de selección de modelo para el problema de clasificación de dos clases múltiples. 
![texto alternativo](https://s3.amazonaws.com/stackabuse/media/understanding-roc-curves-python-1.jpg)

Una curva ROC típica tiene una tasa positiva falsa (FPR) en el eje X y una tasa positiva verdadera (TPR) en el eje Y
"""

# Definimos funcion para trazar la Curva ROC
def plot_roc_curve(fpr, tpr):  
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('Ratio Falsos Positivos (FP)')
    plt.ylabel('Ratio Verdadores Positivos (VP)')
    plt.title('Curva (ROC)')
    plt.legend()
    plt.show()

# Ajustamos el modelo con los datos
model = RandomForestClassifier()  
model.fit(X_train, y_train)

# Predecir las probabilidades para los datos de prueba.
probs = model.predict_proba(X_test)

# Calcular la puntuación AUC.
# Valor 0 Significa que el modelo predice la clase negativa como una clase positiva y viceversa.
auc = roc_auc_score(datostop['y_Actual'], datostop['y_Predicted'])  
print('Valor de AUC: %.2f' % auc)

"""![texto alternativo](https://i2.wp.com/live.staticflickr.com/65535/48048932026_19ee696fdb_c.jpg?resize=800%2C347&ssl=1)"""

# Obtener la curva ROC.
fpr, tpr, thresholds = roc_curve(datostop['y_Actual'], datostop['y_Predicted'])

# Dibujamos la curva ROC
plot_roc_curve(fpr, tpr)